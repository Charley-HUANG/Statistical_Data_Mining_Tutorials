{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1d3c88",
   "metadata": {},
   "source": [
    "***\n",
    "**Tutorial 1 for Chapter 3**\n",
    "\n",
    "A Review of K-means Clustering\n",
    "***\n",
    "\n",
    "<font color = 'darkred'>*Reference:*\n",
    "<font color = 'darkred'>*数据挖掘原理与应用*\n",
    "\n",
    "<!-- Acknowledgement:  \n",
    "**i2DM (Tan, Steinbach, Kumar (2018) Introduction to Data Mining , 2nd Ed, Pearson )** Pearson Press   -->\n",
    "For the course AMA546 Statistical Data Mining   \n",
    "Lecturer: Dr. Catherine Liu    \n",
    "AMA, PolyU, HKSAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95c5efb",
   "metadata": {},
   "source": [
    "**Content:**\n",
    "1. Clustering\n",
    "2.  K-means\n",
    "   - 2.1 The concept of K-means algorithm\n",
    "       - 2.1.1 Centroid\n",
    "       - 2.1.2 Mathematical definition of K-means clustering\n",
    "       - 2.1.3 Method to Find the Best Value of K\n",
    "       - 2.1.4 K-means algorithm and clustering process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b96fce",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd04b5",
   "metadata": {},
   "source": [
    "The **characteristic of clustering** task is that the observed data **only has features without labels**, which is called **unsupervised learning**.\n",
    "\n",
    "***Supervised learning and Unsupervised learning:***   \n",
    " - **Supervised learning** involves the use of **labeled data** to train a model to make predictions or decisions based on new input data. **The goal of supervised learning** is to **minimize the error between the predicted output and the actual output**, also known as the **ground truth**. Common examples of supervised learning applications include **classification** and **regression**.\n",
    "\n",
    " - **Unsupervised learning** involves the analysis of **unlabeled data** to identify patterns or structure within the data, so there is **no ground truth** in the unsupervised learning. In unsupervised learning, the algorithm learns to **identify meaningful patterns or relationships** among the input data, such as **clustering** or **dimensionality reduction**.\n",
    "\n",
    "<img src='https://cdn-coiao.nitrocdn.com/CYHudqJZsSxQpAPzLkHFOkuzFKDpEHGF/assets/static/optimized/rev-85bf93c/wp-content/uploads/2021/04/supervised-learning-data_vs_unsupervised-learning-data.png' width=450>\n",
    "\n",
    "The clustering model needs to divide the observed samples into different groups according to their features. **The goal of cluster analysis** is to make the samples in the **same group have high similarity (minimize the within-cluster variance)** and the objects in **different groups have great divergence (maximize the between-cluster variance)**.\n",
    "\n",
    "<img src='http://claudiaflowers.net/rsch8140/images/cluster_1.png' width=350>\n",
    "\n",
    "Normally, clustering algorithms generally uses the **iterative** technique that **involves trial and failure** to find the best group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf8bfe9",
   "metadata": {},
   "source": [
    "# K-means\n",
    "\n",
    "**K-means** is the most commonly used clustering algorithm. \n",
    " - **Advantages**: **Simple**, easy to **interprete**r, **fast** computation. \n",
    " - **Disadvantages**: It can only be applied to **continuous data** (the centroid of discrete data is not defined), and **the number of clusters** needs to be specified before clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9acb794",
   "metadata": {},
   "source": [
    "## The concept of K-means algorithm\n",
    "The **goal** of the K-means algorithm is to **divide n sample points into k groups**, **each group has a centroid**, and **each point in the group has a shorter distance to the centroid of the group to which it belongs than to the centroid of other groups**. In physics, centroid is the center of gravity of the points, assuming that the weight of each point is equal.\n",
    "\n",
    "### Centroid\n",
    "Centroid is the core concept of K-means clustering algorithm. Centroid is the **central point** obtained by calculating the **mean value of each coordinate of all samples in a group**. The formal definition is stated in **section 2.1.2**.\n",
    "\n",
    "For example, assume we have four points A, B, C and D in $\\mathbf{R}^2$.\n",
    "\n",
    "| Item | x1 | x2 |\n",
    "|------|----|----|\n",
    "| A    | 7  | 9  |\n",
    "| B    | 3  | 3  |\n",
    "| C    | 4  | 7  |\n",
    "| D    | 3  | 8  |\n",
    "\n",
    "Then the cluster centroids, or the **mean of all the variables within the cluster**, are as follows:\n",
    "<img src='centroid.png' width = 300>\n",
    ".\n",
    "\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f31e24c",
   "metadata": {},
   "source": [
    "### Mathematical definition of K-means clustering\n",
    "Given a set of observations $\\left(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\right)$, where each observation is a $d$-dimensional real vector, K-means clustering aims to partition the $n$ observations into $k(\\leq n)$ sets $\\mathbf{S}=\\left\\{S_1, S_2, \\ldots, S_k\\right\\}$ so as to **minimize the within-cluster sum of squares (WCSS)**. Formally, the objective is to find:\n",
    "$$\n",
    "\\underset{\\mathbf{S}}{\\arg \\min } \\sum_{i=1}^k \\sum_{\\mathbf{x} \\in S_i}\\left\\|\\mathbf{x}-\\boldsymbol{\\mu}_i\\right\\|^2\n",
    "$$\n",
    "where $\\boldsymbol{\\mu}_i$ is the **centroid** of points in $S_i$, i.e.\n",
    "$$\n",
    "\\boldsymbol{\\mu}_i=\\frac{1}{\\left|S_i\\right|} \\sum_{\\mathbf{x} \\in S_i} \\mathbf{x}\n",
    "$$\n",
    "$\\left|S_i\\right|$ is the size of $S_i$, and $\\|\\cdot\\|$ is the usual L² norm (Euclidian distance). \n",
    "\n",
    "One can prove that **minimizing the within-cluster sum of squares (WCSS)** is **equivalent** to **maximizing the between-cluster sum of squared (BCSS)** (see [here](https://en.wikipedia.org/wiki/K-means_clustering#Description)). Therefore, the **objective of K-means** clustering is exactly **the same as** the **objective in cluster analysis** we introduced in seciton 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ece868",
   "metadata": {},
   "source": [
    "### Method to Find the Best Value of K\n",
    "\n",
    "The last problem is how to choose the most important parameter: the optimal number of clusters K. Here we will introduce the most common way to **determine the optimal K**: **Elbow Plot Method**.\n",
    "\n",
    "<img src='elbow.png' width =350>\n",
    "\n",
    "Recall that the basic idea behind the k-means clustering is to define clusters such that the **within-cluster sum of squares (WCSS) is minimized**. The total WCSS measures the compactness of the clustering, and we want it to be as small as possible. The elbow method runs K-means clustering on the dataset **for a range of values of K** (say 1 to 10). In the elbow method, we **plot WCSS with respect to K** and look for the **elbow point where the rate of decrease shifts**, that is, the decreasing rate of WCSS is fast before elbow point and is slow after elbow point.\n",
    "\n",
    "In the Elbow plot above, the elbow point appears in K=3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c4e21",
   "metadata": {},
   "source": [
    "### K-means algorithm and clustering process\n",
    "The following algorithm is the **K-means algorithm** we taught in class. We will explain the algorithm with illustrations to understand the process of K-means clustering.\n",
    "<img src='algo.png' width =700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96106bce",
   "metadata": {},
   "source": [
    "1. We have **9 yellow sample points** in the figure below, and let's say we want to divide it into **3 clusters**, so the K value of the K-means is set to 3:\n",
    "<img src='s1.png' width =300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a79fc4",
   "metadata": {},
   "source": [
    "2. Following the first line in the algorithm, we **randomly select three points** (the three in the bottom left corner) and mark them blue: (*you can also select points other than the sample points in the space*)\n",
    "\n",
    "<img src='s2.png' width =300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c991aafd",
   "metadata": {},
   "source": [
    "3. And then, we calculate the **distance from each yellow point to each blue point**, determine **which blue point is closest to each yellow point**, and **group** them together (line 3 in the algorithm). Clusters are circled in red:\n",
    "\n",
    "<img src='s3.png' width =300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914cbf78",
   "metadata": {},
   "source": [
    "4. After that, we **recompute the centroid of each cluster** (line 4 in the algorithm). The formula of the centroid is stated above. The new centroid are colored in blue and the arrow shows the movement of the centroid:\n",
    "\n",
    "<img src='s4.png' width =300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7118e8",
   "metadata": {},
   "source": [
    "5. The first cycle is over. Since the centroid changed, we **start the second loop** (line 5 in the algorithm). We **assign all points to the cloest centroid** like what we did in step 3 (line 3 in the algorithm):\n",
    "<img src='s5.png' width =300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48075ae",
   "metadata": {},
   "source": [
    "6. Then we **recompute the centroid of each cluster** (line 4 in the algorithm):\n",
    "<img src='s6.png' width =300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618cd6a2",
   "metadata": {},
   "source": [
    "7. The second cycle is over. Since the centroid changed, we **start the third loop** (line 5 in the algorithm). We **assign all points to the cloest centroid**. As you can see, all the points are assigned to the same group, and the **centroid doesn't change any more**. Therefore, K-means reaches a state in which no points are shifting from one cluster to another (line 5 in the algorithm) and **terminated**. \n",
    "<img src='s7.png' width =300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f067dd0d",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "This is **K-means clustering**, and the whole idea is to **minimize the distance between the sample point and the center of mass**. \n",
    "\n",
    "We first **randomly initializing the centroid points**, then we iterated the following two processes: \n",
    "1. Clustering the sample points with the **shortest distance from the centroid points** into a group; \n",
    "2. Use the center point of the group as the **new centroid point**  \n",
    "\n",
    "until the centroid **no longer changes**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "64"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
