{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "504044e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-04T07:21:30.723223Z",
     "start_time": "2023-03-04T07:21:30.711852Z"
    }
   },
   "source": [
    "***\n",
    "**Tutorial 6 for Chapter 2**\n",
    "\n",
    "Case study 8: Movie Recommendation by Singular Value Decomposition\n",
    "***\n",
    "<font color = 'darkred'>*Reference:*\n",
    "<font color = 'darkred'>*数据挖掘原理与应用*\n",
    "\n",
    "<!-- Acknowledgement:  \n",
    "**i2DM (Tan, Steinbach, Kumar (2018) Introduction to Data Mining , 2nd Ed, Pearson )** Pearson Press   -->\n",
    "For the tutorial course of AMA546 Statistical Data Mining   \n",
    "Lecturer: Mr. Qiuyi Huang\n",
    "PolyU, HKSAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a662e8ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T13:39:31.103020Z",
     "start_time": "2023-03-05T13:39:29.873213Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e464ffb",
   "metadata": {},
   "source": [
    "**Content:**\n",
    "- Definition of SVD\n",
    "- Computing the SVD of a Matrix\n",
    "- Example 1: Feature Extraction by SVD\n",
    "- Example 2: A More Complicated Case\n",
    "- Example 3: Dimensionality Reduction by SVD\n",
    "- Example 4: Movie Recommendation Using Features\n",
    "    - Recommend movie\n",
    "        - Quincy\n",
    "        - Leslie\n",
    "    - Recommend user\n",
    "        - Quincy\n",
    "        - Leslie\n",
    "- Appendix: Why truncated SVD work?\n",
    "- Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a7ac72",
   "metadata": {},
   "source": [
    "We now take up a form of matrix analysis that leads to a **low-dimensional representation** of a high-dimensional matrix. This approach, called **singular value decomposition (SVD)**, not only allows an **exact representation** of any matrix, but also makes it easy to **eliminate the less important parts** of that representation to produce an approximate representation with any desired number of dimensions. The **fewer the dimensions** we choose, the **less complexity but less accurate** will be the approximation.\n",
    "\n",
    "We begin with the necessary definitions. Then, we explore the idea that the SVD defines a small number of **features** that connect the rows (users) and columns (items) of the matrix. We show how eliminating the least important features gives us a smaller representation that closely approximates the original matrix. Final, we see how these features facilitate the movie recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b0f599",
   "metadata": {},
   "source": [
    "# Definition of SVD\n",
    "\n",
    "Let $M$ be an $m×n$ matrix, and let the rank of $M$ be $r$. Recall that the **rank of a matrix** is the largest number of rows (or columns) we can choose for which **NO nonzero linear combination of the rows (or columns) is the all-zero vector 0** (we say a set of such rows or columns is independent). Then we can find matrices $U$, $Σ$, and $V$ as shown in figure below with the following properties:\n",
    "\n",
    "<img src='SVD_original_def.png' width=500>\n",
    "\n",
    " - $U$ is an $m \\times m$ column-orthonormal matrix ; that is, each of its columns is a unit vector and the dot product of any two columns is 0.\n",
    " - $V$ is an $n \\times n$ column-orthonormal matrix. Note that we always use $V$ in its transposed form, so it is the rows of $V^T$ that are orthonormal.\n",
    " - $\\Sigma$ is an $m \\times n$ rectangular diagonal matrix; that is, all elements not on the main diagonal are 0. The diagnal elements of $\\Sigma$ are called the singular values of M.\n",
    "\n",
    "Since the **first $r$ diagnal items of $\\Sigma$ are non-zero**, only the **first $r$ columns in $U$** and **first $r$ rows in $V^T$** are in effect. Therefore, we can derive a **equivalent** definition of SVD:\n",
    "\n",
    "<img src='svd_def_equ.png' width=500>\n",
    "\n",
    " - $U$ is an $m \\times r$ column-orthonormal matrix ; that is, each of its columns is a unit vector and the dot product of any two columns is 0.\n",
    " - $V$ is an $r \\times n$ column-orthonormal matrix. Note that we always use $V$ in its transposed form, so it is the rows of $V^T$ that are orthonormal.\n",
    " - $\\Sigma$ is an $r \\times r$ diagonal matrix. The diagnal elements of $\\Sigma$ are called the singular values of M."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596b56ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T02:46:21.208755Z",
     "start_time": "2023-03-05T02:46:21.183113Z"
    }
   },
   "source": [
    "# Computing the SVD of a Matrix\n",
    "The SVD of a matrix $M$ is strongly connected to the eigenvalues of the symmetric matrices $M^{\\mathrm{T}} M$ and $M M^{\\mathrm{T}}$. This relationship allows us to obtain the SVD of $M$ from the eigenpairs of the latter two matrices. To begin the explanation, start with $M=U \\Sigma V^{\\mathrm{T}}$, the expression for the SVD of $M$. Then\n",
    "\n",
    "\\begin{align*}\n",
    "M^{\\mathrm{T}}=\\left(U \\Sigma V^{\\mathrm{T}}\\right)^{\\mathrm{T}}=\\left(V^{\\mathrm{T}}\\right)^{\\mathrm{T}} \\Sigma^{\\mathrm{T}} U^{\\mathrm{T}}=V \\Sigma^{\\mathrm{T}} U^{\\mathrm{T}}\n",
    "\\end{align*}\n",
    "\n",
    "Since $\\Sigma$ is a diagonal matrix, transposing it has no effect. Thus, $M^{\\mathrm{T}}=V \\Sigma U^{\\mathrm{T}}$. Now, $M^{\\mathrm{T}} M=V \\Sigma U^{\\mathrm{T}} U \\Sigma V^{\\mathrm{T}}$. Remember that $U$ is an orthonormal matrix, so $U^{\\mathrm{T}} U$ is the identity matrix of the appropriate size. That is,\n",
    "\n",
    "\\begin{align*}\n",
    "M^{\\mathrm{T}} M=V \\Sigma^2 V^{\\mathrm{T}}\n",
    "\\end{align*}\n",
    "\n",
    "Multiply both sides of this equation on the right by $V$ to get\n",
    "\n",
    "\\begin{align*}\n",
    "M^{\\mathrm{T}} M V=V \\Sigma^2 V^{\\mathrm{T}} V\n",
    "\\end{align*}\n",
    "\n",
    "Since $V$ is also an orthonormal matrix, we know that $V^{\\mathrm{T}} V$ is the identity. Thus\n",
    "\n",
    "\\begin{align*}\n",
    "M^{\\mathrm{T}} M V=V \\Sigma^2\n",
    "\\end{align*}\n",
    "\n",
    "Since $\\Sigma$ is a diagonal matrix, $\\Sigma^2$ is also a diagonal matrix whose entry in the $i$ th row and column is the square of the entry in the same position of $\\Sigma$. It says that **$V$ is the matrix of eigenvectors of $M^{\\mathrm{T}} M$** and **$\\Sigma^2$ is the diagonal matrix whose entries are the corresponding eigenvalues**.\n",
    "\n",
    "Thus, the same algorithm that computes the eigenpairs for $M^{\\mathrm{T}} M$ gives us the matrix $V$ for the SVD of $M$ itself. It also gives us the singular values for this SVD; just take the square roots of the eigenvalues for $M^{\\mathrm{T}} M$.\n",
    "\n",
    "Only $U$ remains to be computed, but it can be found in the same way we found $V$. Start with\n",
    "\n",
    "\\begin{align*}\n",
    "M M^{\\mathrm{T}}=U \\Sigma V^{\\mathrm{T}}\\left(U \\Sigma V^{\\mathrm{T}}\\right)^{\\mathrm{T}}=U \\Sigma V^{\\mathrm{T}} V \\Sigma U^{\\mathrm{T}}=U \\Sigma^2 U^{\\mathrm{T}}\n",
    "\\end{align*}\n",
    "\n",
    "Then by a series of manipulations analogous to the above, we learn that\n",
    "\n",
    "\\begin{align*}\n",
    "M M^{\\mathrm{T}} U=U \\Sigma^2\n",
    "\\end{align*}\n",
    "\n",
    "That is, **$U$ is the matrix of eigenvectors of $M M^{\\mathrm{T}}$**.\n",
    "A small detail needs to be explained concerning $U$ and $V$. Each of these matrices have $r$ columns, while $M^{\\mathrm{T}} M$ is an $n \\times n$ matrix and $M M^{\\mathrm{T}}$ is an $m \\times m$ matrix. Both $n$ and $m$ are at least as large as $r$. Thus, $M^{\\mathrm{T}} M$ and $M M^{\\mathrm{T}}$ should have an additional $n-r$ and $m-r$ eigenpairs, respectively, and these pairs do not show up in $U, V$, and $\\Sigma$. Since the rank of $M$ is $r$, all other eigenvalues will be 0 , and these are not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e28e8",
   "metadata": {},
   "source": [
    "# Example 1: Feature Extraction by SVD\n",
    "\n",
    "Here gives a **rank-2 matrix** representing ratings of movies by users. In this contrived example there are **two “feature”** underlying the movies: **science-fiction and romance**. **All the boys rate only science-fiction, and all the girls rate only romance.** Our objective is to **recommend movies to customers based on their rateings** of movies via the Singular Value Decomposition.\n",
    "\n",
    "|       | Matrix | Alien | Star Wars | Casablanca | Titanic |\n",
    "|-------|--------|-------|-----------|------------|---------|\n",
    "| Joe   | 1      | 1     | 1         | 0          | 0       |\n",
    "| Jim   | 3      | 3     | 3         | 0          | 0       |\n",
    "| John  | 4      | 4     | 4         | 0          | 0       |\n",
    "| Jack  | 5      | 5     | 5         | 0          | 0       |\n",
    "| Jill  | 0      | 0     | 0         | 4          | 4       |\n",
    "| Jenny | 0      | 0     | 0         | 5          | 5       |\n",
    "| Jane  | 0      | 0     | 0         | 2          | 2       |\n",
    "\n",
    "It is this existence of **two concepts that gives the matrix a rank of 2**. That is, we may pick one of the first four rows and one of the last three rows and observe that there is no nonzero linear sum of these rows that is 0. But we cannot pick three independent rows. For example, if we pick rows 1, 2, and 7, then three times the first minus the second, plus zero times the seventh is 0. We can make a similar observation about the columns. We may pick one of the first three columns and one of the last two coluns, and they will be independent, but no set of three columns is independent.\n",
    "\n",
    "The **matrix $M$ can be decomposed into $U$ , $\\Sigma$, and $V$**,** with all elements correct to two significant digits, is shown in below. Since the rank of M is 2, we can use r = 2 in the decomposition.    We have already see how to compute this decomposition in Section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f27fb3c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T13:45:58.534753Z",
     "start_time": "2023-03-05T13:45:58.520797Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "x1 = np.matrix([[1,1,1,0,0],\n",
    "                [3,3,3,0,0],\n",
    "                [4,4,4,0,0],\n",
    "                [5,5,5,0,0],\n",
    "                [0,0,0,4,4],\n",
    "                [0,0,0,5,5],\n",
    "                [0,0,0,2,2]])\n",
    "# singular value deconposition to the matrix\n",
    "u1, sigma1, v1 = np.linalg.svd(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be1757ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T13:45:58.770117Z",
     "start_time": "2023-03-05T13:45:58.762530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7) \n",
      " [[ 0.14 -0.   -0.42  0.56  0.42 -0.52 -0.21]\n",
      " [ 0.42 -0.   -0.15  0.21 -0.85 -0.19 -0.08]\n",
      " [ 0.56 -0.   -0.21 -0.72  0.21 -0.26 -0.1 ]\n",
      " [ 0.7  -0.    0.34  0.34  0.26  0.42  0.17]\n",
      " [-0.    0.6  -0.64 -0.   -0.    0.44  0.18]\n",
      " [-0.    0.75  0.44 -0.   -0.   -0.44  0.22]\n",
      " [-0.    0.3   0.18 -0.   -0.    0.22 -0.91]] \n",
      "\n",
      "(5,) \n",
      " [12.37  9.49  0.    0.    0.  ] \n",
      "\n",
      "(5, 5) \n",
      " [[ 0.58  0.58  0.58 -0.   -0.  ]\n",
      " [ 0.    0.    0.    0.71  0.71]\n",
      " [-0.   -0.   -0.    0.71 -0.71]\n",
      " [-0.    0.71 -0.71 -0.   -0.  ]\n",
      " [-0.82  0.41  0.41 -0.   -0.  ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the dimension and result of the decomposition result\n",
    "for i in [-u1, sigma1, -v1]:\n",
    "    print(i.shape, '\\n',  i.round(2), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ca949",
   "metadata": {},
   "source": [
    "<!-- **The same as the textbook:**\n",
    "<img src='svd.png' width = 600> -->\n",
    "***The decomposition result can be written into this form:***\n",
    "\n",
    "\\[\n",
    "\\underbrace{\\left[\\begin{array}{lllll}\n",
    "1 & 1 & 1 & 0 & 0 \\\\\n",
    "3 & 3 & 3 & 0 & 0 \\\\\n",
    "4 & 4 & 4 & 0 & 0 \\\\\n",
    "5 & 5 & 5 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 4 & 4 \\\\\n",
    "0 & 0 & 0 & 5 & 5 \\\\\n",
    "0 & 0 & 0 & 2 & 2\n",
    "\\end{array}\\right]}_{M} = \\underbrace{\\left[\\begin{array}{cc}\n",
    "0.14 & 0 \\\\\n",
    "0.42 & 0 \\\\\n",
    "0.56 & 0 \\\\\n",
    "0.70 & 0 \\\\\n",
    "0 & 0.60 \\\\\n",
    "0 & 0.75 \\\\\n",
    "0 & 0.30\n",
    "\\end{array}\\right]}_{U}  \\underbrace{\\left[\\begin{array}{cc}\n",
    "12.4 & 0 \\\\\n",
    "0 & 9.5\n",
    "\\end{array}\\right]}_{\\Sigma}  \\underbrace{\\left[\\begin{array}{ccccc}\n",
    "0.58 & 0.58 & 0.58 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0.71 & 0.71\n",
    "\\end{array}\\right]}_{V^T}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fb8940",
   "metadata": {},
   "source": [
    "***Interpretation of SVD:***\n",
    "\n",
    "<img src='svd_feature.png' width=600>\n",
    "\n",
    "The key to understanding what SVD offers is in viewing **the $r$ columns of $U$, $\\Sigma$, and $V$ as representing unobservable features** that are hidden in the original matrix $M$. In the example, these features are clear; one is \"**science fiction**\" and the other is \"**romance**\". Let us think of the rows of $M$ as people and the columns of $M$ as movies:\n",
    "\n",
    " - The **matrix $U$ connects people to features**. For example, the person Joe, who corresponds to row 1 of $M$, likes only the feature science fiction. The value 0.14 in the first row and first column of $U$ is smaller than some of the other entries in that column, because while Joe watches only science fiction, he doesn't rate those movies highly. The second column of the first row of $U$ is 0 , because Joe doesn't rate romance movies at all.\n",
    "\n",
    " - The **matrix $V$ relates movies to features**. The 0.58 in each of the first three columns of the first row of $V^{\\mathrm{T}}$ indicates that the first three movies (The Matrix, Alien, and Star Wars) each are science-fiction, while the 0's in the last two columns of the first row say that these movies do not relate to the concept romance at all. Likewise, the second row of $V^{\\mathrm{T}}$ tells us that the movies Casablanca and Titanic are exclusively romances.\n",
    "\n",
    " - The **matrix $\\Sigma$ gives the strength of each of the concepts in the dataset**. In our example, the strength of the science-fiction concept is 12.4 , while the strength of the romance concept is 9.5 . Intuitively, the science-fiction concept is stronger because the data provides more information about the movies of that  people who like them.\n",
    "\n",
    "***Gerneral case:***\n",
    "\n",
    "In general, **the concepts will not be so clear**.There will be **fewer 0's in $U$ and $V$**, although $\\Sigma$ is always a diagonal matrix and will always have 0's off the diagonal. The entities represented by the rows and columns of $M$ (analogous to people and movies in our example) will take up several different concepts. In practice, when the rank of $M$ is greater than the number of columns we want for the matrices $U, \\Sigma$, and $V$, the decomposition is not exact. We need to **eliminate from the exact decomposition those columns of $U$ and $V$ that correspond to the smallest singular values**, in order to get the best approximation. The following example is a slight modification of the example that will illustrate the point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e28561f",
   "metadata": {},
   "source": [
    "# Example 2: A More Complicated Feature Extraction\n",
    "The dataset below is almost the same as the one above, **but Jill and Jane rated Alien**, although neither liked it very much. The rank of the matrix in here is 3; for example the first, sixth, and seventh rows are independent, but you can check that no four rows are independent. \n",
    "\n",
    "|       | Matrix | Alien | Star Wars | Casablanca | Titanic |\n",
    "|-------|--------|-------|-----------|------------|---------|\n",
    "| Joe   | 1      | 1     | 1         | 0          | 0       |\n",
    "| Jim   | 3      | 3     | 3         | 0          | 0       |\n",
    "| John  | 4      | 4     | 4         | 0          | 0       |\n",
    "| Jack  | 5      | 5     | 5         | 0          | 0       |\n",
    "| Jill  | 0      | <font color='red'>**2**    | 0         | 4          | 4       |\n",
    "| Jenny | 0      | 0     | 0         | 5          | 5       |\n",
    "| Jane  | 0      | <font color='red'>**1**     | 0         | 2          | 2       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1fd41b",
   "metadata": {},
   "source": [
    "Here we perform the SVD decomposition to the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f23b96c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T13:46:08.284723Z",
     "start_time": "2023-03-05T13:46:08.277363Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "x2 = np.matrix([[1,1,1,0,0],\n",
    "                [3,3,3,0,0],\n",
    "                [4,4,4,0,0],\n",
    "                [5,5,5,0,0],\n",
    "                [0,2,0,4,4],\n",
    "                [0,0,0,5,5],\n",
    "                [0,1,0,2,2]])\n",
    "# svd\n",
    "u2, sigma2, v2 = np.linalg.svd(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c7ae581",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T13:46:09.706007Z",
     "start_time": "2023-03-05T13:46:09.696284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7) \n",
      " [[ 0.14 -0.02  0.01  0.56 -0.38 -0.7  -0.19]\n",
      " [ 0.41 -0.07  0.03  0.21  0.76 -0.26  0.38]\n",
      " [ 0.55 -0.09  0.04 -0.72 -0.18 -0.34 -0.09]\n",
      " [ 0.69 -0.12  0.05  0.34 -0.23  0.57 -0.12]\n",
      " [ 0.15  0.59 -0.65  0.   -0.2  -0.    0.4 ]\n",
      " [ 0.07  0.73  0.68 -0.   -0.   -0.   -0.  ]\n",
      " [ 0.08  0.3  -0.33  0.    0.4  -0.   -0.8 ]] \n",
      "\n",
      "(5,) \n",
      " [12.48  9.51  1.35  0.    0.  ] \n",
      "\n",
      "(5, 5) \n",
      " [[ 0.56  0.59  0.56  0.09  0.09]\n",
      " [-0.13  0.03 -0.13  0.7   0.7 ]\n",
      " [ 0.41 -0.8   0.41  0.09  0.09]\n",
      " [-0.71  0.    0.71  0.   -0.  ]\n",
      " [-0.    0.   -0.    0.71 -0.71]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the dimension and result of the decomposition result\n",
    "for i in [-u2, sigma2, -v2]:\n",
    "    print(i.shape, '\\n',  i.round(2), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8202ff3",
   "metadata": {},
   "source": [
    "***The decomposition result can be written into this form:***\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "\\underbrace{\\left[\\begin{array}{lllll}\n",
    "1 & 1 & 1 & 0 & 0 \\\\\n",
    "3 & 3 & 3 & 0 & 0 \\\\\n",
    "4 & 4 & 4 & 0 & 0 \\\\\n",
    "5 & 5 & 5 & 0 & 0 \\\\\n",
    "0 & 2 & 0 & 4 & 4 \\\\\n",
    "0 & 0 & 0 & 5 & 5 \\\\\n",
    "0 & 1 & 0 & 2 & 2\n",
    "\\end{array}\\right]}_{M^ \\prime} = \\underbrace{\\left[\\begin{array}{rrr}\n",
    ".13 & .02 & -.01 \\\\\n",
    ".41 & .07 & -.03 \\\\\n",
    ".55 & .09 & -.04 \\\\\n",
    ".68 & .11 & -.05 \\\\\n",
    ".15 & -.59 & .65 \\\\\n",
    ".07 & -.73 & -.67 \\\\\n",
    ".07 & -.29 & .32\n",
    "\\end{array}\\right]}_{U^ \\prime}  \\underbrace{\\left[\\begin{array}{ccc}\n",
    "12.4 & 0 & 0 \\\\\n",
    "0 & 9.5 & 0 \\\\\n",
    "0 & 0 & 1.3\n",
    "\\end{array}\\right]}_{\\Sigma^ \\prime}  \\underbrace{\\left[\\begin{array}{rrrrrr}\n",
    ".56 & .59 & .56 & .09 & .09 \\\\\n",
    ".12 & -.02 & .12 & -.69 & -.69 \\\\\n",
    ".40 & -.80 & .40 & .09 & .09\n",
    "\\end{array}\\right]}_{{V ^\\prime} ^T}\n",
    "\\end{aligned}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ecbb0",
   "metadata": {},
   "source": [
    "**Only three columns for $U^\\prime$, $\\Sigma^\\prime$, and $V^\\prime$ need to be included, because the matrix is of rank three**. The columns of $U$ and $V$ still correspond to features. The first is still \"science fiction\" and the second is \"romance.\" **It is harder to tell the third column's feature**, but it doesn't matter all that much, because **its weight**, as given by the **third nonzero entry in $\\Sigma$**, **is very low** compared with the weights of the first two concepts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da068385",
   "metadata": {},
   "source": [
    "# Example 2 cont.: Dimensionality Reduction by SVD\n",
    "In this section, we consider **eliminating some of the least important features**. For instance, we might want to **eliminate the third feature** in Example 2, since it tells little about the dataset, and the fact that its **associated singular value is so small** confirms its unimportance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef617d0b",
   "metadata": {},
   "source": [
    "Dimensionality Reduction Using SVD\n",
    "Suppose we want to **represent a very large matrix $M$ by its SVD components** $U$, $\\Sigma$, and $V$, but these matrices are also too large to store conveniently. The best way to **reduce the dimensionality** of the three matrices is to **set the smallest of the singular values to zero**. If we set the $s$ smallest singular values to 0 , then we can **also eliminate the corresponding $s$ columns of $U$ and $V$**.\n",
    "\n",
    "The decomposition of **Example 2 has three singular values**. Suppose we want to reduce the number of dimensions to two. Then we **set the smallest of the singular values to zero.** The effect is that the **third column of $U^ \\prime$** and the **third row of ${V^ \\prime}^{T}$** are **multiplied only by 0's** when we perform the multiplication, so this row and this column may as well not be there. That is, the approximation to $M^{\\prime}$ obtained by using only the two largest singular values:\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "\\underbrace{\\left[\\begin{array}{lllll}\n",
    "1 & 1 & 1 & 0 & 0 \\\\\n",
    "3 & 3 & 3 & 0 & 0 \\\\\n",
    "4 & 4 & 4 & 0 & 0 \\\\\n",
    "5 & 5 & 5 & 0 & 0 \\\\\n",
    "0 & 2 & 0 & 4 & 4 \\\\\n",
    "0 & 0 & 0 & 5 & 5 \\\\\n",
    "0 & 1 & 0 & 2 & 2\n",
    "\\end{array}\\right]}_{M^ \\prime} &= \\underbrace{\\left[\\begin{array}{rrr}\n",
    ".13 & .02 & -.01 \\\\\n",
    ".41 & .07 & -.03 \\\\\n",
    ".55 & .09 & -.04 \\\\\n",
    ".68 & .11 & -.05 \\\\\n",
    ".15 & -.59 & .65 \\\\\n",
    ".07 & -.73 & -.67 \\\\\n",
    ".07 & -.29 & .32\n",
    "\\end{array}\\right]}_{U^ \\prime}  \\underbrace{\\left[\\begin{array}{ccc}\n",
    "12.4 & 0 & 0 \\\\\n",
    "0 & 9.5 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{array}\\right]}_{\\Sigma^ \\prime}  \\underbrace{\\left[\\begin{array}{rrrrrr}\n",
    ".56 & .59 & .56 & .09 & .09 \\\\\n",
    ".12 & -.02 & .12 & -.69 & -.69 \\\\\n",
    ".40 & -.80 & .40 & .09 & .09\n",
    "\\end{array}\\right]}_{{V ^\\prime} ^T} \\\\\n",
    "&= \n",
    "{\\left[\\begin{array}{rr}\n",
    ".13 & .02 \\\\\n",
    ".41 & .07 \\\\\n",
    ".55 & .09 \\\\\n",
    ".68 & .11 \\\\\n",
    ".15 & -.59 \\\\\n",
    ".07 & -.73 \\\\\n",
    ".07 & -.29\n",
    "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
    "12.4 & 0 \\\\\n",
    "0 & 9.5\n",
    "\\end{array}\\right]\\left[\\begin{array}{rrrrr}\n",
    ".56 & .59 & .56 & .09 & .09 \\\\\n",
    ".12 & -.02 & .12 & -.69 & -.69\n",
    "\\end{array}\\right]} \\\\\n",
    "& =\\left[\\begin{array}{lllll}\n",
    "0.93 & 0.95 & 0.93 & .014 & .014 \\\\\n",
    "2.93 & 2.99 & 2.93 & .000 & .000 \\\\\n",
    "3.92 & 4.01 & 3.92 & .026 & .026 \\\\\n",
    "4.84 & 4.96 & 4.84 & .040 & .040 \\\\\n",
    "0.37 & 1.21 & 0.37 & 4.04 & 4.04 \\\\\n",
    "0.35 & 0.65 & 0.35 & 4.87 & 4.87 \\\\\n",
    "0.16 & 0.57 & 0.16 & 1.98 & 1.98\n",
    "\\end{array}\\right]\n",
    "\\end{aligned}\n",
    "\n",
    "The resulting matrix is quite close to the original matrix $M^{\\prime}$. Ideally, the entire difference is the result of making the last singular value be 0. However, in this simple example, **much of the difference is due to rounding error** caused by the fact that the decomposition of $M^{\\prime}$ was **only correct to two significant digits**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67ce87",
   "metadata": {},
   "source": [
    "# Example 2 cont.: Movie Recommendation Using Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f62939",
   "metadata": {},
   "source": [
    "In this section we shall look at how SVD can help us recommend the movies efficiently, with good accuracy. We make movie recommendation based on the following assumption: if a person **likes a certain movie**, then he **will also like the same type of movie**.\n",
    "\n",
    "## Recommend movie\n",
    "### Quincy\n",
    "\n",
    "Let us make recommendation **based on the first two features in Example 3**. Quincy is not one of the people represented by the original matrix, but he wants to use the system to know what movies he would like. He has only seen one movie, The Matrix, and rated it 4 . Thus, we can represent Quincy by the vector $\\mathbf{q}=[4,0,0,0,0]$, as if this were one of the rows of the original matrix.\n",
    "\n",
    "We can **map Quincy into \"feature space\"** by multiplying him by the matrix $V$ of the decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227b3b1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T13:39:31.130907Z",
     "start_time": "2023-03-05T13:39:31.124571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 2.24903362, -0.50656553]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quincy\n",
    "M_Quincy = np.matrix([4,0,0,0,0]) # rateing matrix of Quincy\n",
    "Quincy_feature = M_Quincy.dot(np.transpose(-v2[:2])) # use the first two columns\n",
    "Quincy_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a7ced5",
   "metadata": {},
   "source": [
    "We find $\\mathbf{q} V=[2.25,-0.51].$ That is to say, **Quincy is high in science-fiction interest**, and not very interested in romance.\n",
    "\n",
    "We now have a **representation of Quincy in feature space**, derived from, but different from his representation in the **original \"movie space**\". One useful thing we can do is to **map his representation back into movie space** by multiplying $[2.25,-0.51]$ by $V^{\\mathrm{T}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "881286aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T13:39:31.134994Z",
     "start_time": "2023-03-05T13:39:31.131767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.32869022,  1.31878766,  1.32869022, -0.14954027, -0.14954027]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Quincy_feature.dot(-v2[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b33fa",
   "metadata": {},
   "source": [
    "This product is $[1.33,1.32,1.33,-0.15,-0.15]$. Thus, **the SVD decomposition suggests that Quincy would like Alien and Star Wars, but not Casablanca or Titanic**.\n",
    "\n",
    "### Leslie\n",
    "\n",
    "Suppose Leslie assigns **rating 3 to Alien and rating 4 to Titanic**, giving us a representation of Leslie in \"movie space\" of $[0,3,0,0,4]$. Then the representation of Leslie in concept space is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "171d076d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T13:39:31.138961Z",
     "start_time": "2023-03-05T13:39:31.135747Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1.73205081, 2.82842712]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Leslie\n",
    "M_Leslie = np.matrix([0,3,0,0,4])\n",
    "Leslie_feature = M_Leslie.dot(np.transpose(-v1[:2]))\n",
    "Leslie_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1364251",
   "metadata": {},
   "source": [
    "$\\mathbf{q} V=[1.73,2.83]$.  **It seems Leslie prefer romance more than science-fiction.**\n",
    "\n",
    "Then the predicted Leslie's rating of the movie is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42768a6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T13:39:31.143041Z",
     "start_time": "2023-03-05T13:39:31.139663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.61566421, 1.10823897, 0.61566421, 2.12293683, 2.12293683]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Leslie_feature.dot(-v2[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4085a1",
   "metadata": {},
   "source": [
    "For Leslie, the ratings on Casablanca or Titanic may higher than them on Matrix, Alien and Star Wars. Since Leslie has already watched the Titanic, it suggests that **Leslie will prefer to watch Casablanca**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa5201",
   "metadata": {},
   "source": [
    "## Recommend user\n",
    "### Quincy\n",
    "\n",
    "Apart from the movie recommendation, we can also **find users similar to Quincy** in concept space. In other words, we can **recommend users with similar movie tastes to Quincy**. We can use $V$ to map all users in the dataset into concept space: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bdc7668",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T13:39:31.150945Z",
     "start_time": "2023-03-05T13:39:31.147215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.71737671, -0.22451218],\n",
       "        [ 5.15213013, -0.67353654],\n",
       "        [ 6.86950685, -0.89804872],\n",
       "        [ 8.58688356, -1.12256089],\n",
       "        [ 1.9067881 ,  5.62055093],\n",
       "        [ 0.90133537,  6.9537622 ],\n",
       "        [ 0.95339405,  2.81027546]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2_feature = x2.dot(np.transpose(-v2[:2]))\n",
    "x2_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3978b",
   "metadata": {},
   "source": [
    "Joe (first row) maps to $[1.72,-0.22]$, and Jane (last row) maps to $[0.95,2.81]$. Then, we can measure the similarity of users by their **cosine distance in concept space**. \n",
    "\n",
    "The **cosine distance** and the code below has introduced in *Tutorial on Chapter 1 Data: type, quality, dis/similarity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90b9625c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T13:39:31.154746Z",
     "start_time": "2023-03-05T13:39:31.152178Z"
    }
   },
   "outputs": [],
   "source": [
    "def COSDistance(x,y):\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"Undefined for sequences of unequal length\")\n",
    "    x = np.array(x); y = np.array(y)\n",
    "    return (x @ np.transpose(y)) / (np.linalg.norm(x)*np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74f5714a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T13:39:31.159429Z",
     "start_time": "2023-03-05T13:39:31.155551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995812457586508\n",
      "0.9958124575865079\n",
      "0.995812457586508\n",
      "0.995812457586508\n",
      "0.10533291005166293\n",
      "-0.09250782773698496\n",
      "0.10533291005166293\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cos distance between people in dataset and Quincy\n",
    "for people in np.arange(0, len(x2_feature)):\n",
    "    print(COSDistance(x2_feature[people], Quincy_feature)[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f9e5a",
   "metadata": {},
   "source": [
    "For the case introduced above, note that the **concept vectors for Quincy and boys (first four rows)**, are **almost the same**. That is, their cosine distance is approximately 1. On the other hand, the **vectors for Quincy and girls (last three rows)**, have a **dot product of 0**, and therefore their angle is about 90 degrees. Therefore, we can **recommend those male users to Quincy** and they may become friends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d751c",
   "metadata": {},
   "source": [
    "### Leslie\n",
    "Just replace the feature vector of Quincy by that of Leslie, then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c26a246",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-05T13:39:31.164441Z",
     "start_time": "2023-03-05T13:39:31.161243Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40728076375769534\n",
      "0.40728076375769523\n",
      "0.40728076375769534\n",
      "0.40728076375769534\n",
      "0.9753711989030948\n",
      "0.9128573731289783\n",
      "0.9753711989030948\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cos distance between people in dataset and Quincy\n",
    "for people in np.arange(0, len(x2_feature)):\n",
    "    print(COSDistance(x2_feature[people], Leslie_feature)[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634edded",
   "metadata": {},
   "source": [
    "Therefore, **Leslie share the same movie tastes with the girls (last three rows)**, especilally the Jill (row 5) and Jane (row 7)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29c7756",
   "metadata": {},
   "source": [
    "# Appendix: Why does truncated SVD work?\n",
    "In **truncated SVD**, the diagonal matrix $\\Sigma$ is truncated by **removing the singular values below a certain threshold or beyond a certain rank**, which reduces the number of columns in $U$ and rows in $V^T$. The resulting truncated matrices are then used to **reconstruct an approximation ([Low-rank approximation](https://en.wikipedia.org/w/index.php?search=Low-rank+approximation&title=Special%3ASearch&wprov=acrw1_-1)) of the original matrix**. The benefit of truncated SVD is that it can **reduce the computational complexity and storage requirements** of the full SVD algorithm, while still providing a good approximation of the original matrix.\n",
    "\n",
    "Actually, by preserving the a few largest singular values and set the other to zero, the **[Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm) $\\left(\\|A\\|_{\\mathrm{F}}=\\sqrt{\\sum_i^m \\sum_j^n\\left|a_{i j}\\right|^2}\\right)$** between the reconstructed matrix and original matrix is **minimized**. The result is referred to as the **matrix approximation lemma or Eckart–Young–Mirsky theorem**. Here we provide a brief proof:\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Let\n",
    "\n",
    "\\begin{align*}\n",
    "M=U \\Sigma V^{\\prime}\n",
    "\\end{align*}\n",
    "\n",
    "be the SVD of the $n \\times r$ matrix $M$. \n",
    "\n",
    "Since the **Frobenius norm** is invariant **under left- and right-multiplication by orthogonal matrices**, since **orthogonality by definition means preservation of the Euclidean norm** and the **Frobenius norm** (when squared) is both (a) **the sum of squared Euclidean norms of the rows** (and so is invariant under left multiplication, which preserves each row norm) and (b) **the sum of squared Euclidean norms of the columns** (and so is invariant under right multiplication, which preserves each column norm).\n",
    "\n",
    "Therefore, for the **Frobenius norm**, whenever $\\boldsymbol{P}$ is an $n \\times n$ orthogonal matrix or $Q$ is an $r \\times r$ orthogonal matrix, then\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\|P^{\\prime} M Q\\right\\|=\\|M\\|\n",
    "\\end{align*}\n",
    "\n",
    "Let $A$ be the matrix reconstructed by the **some $k$ singular values**. Then, by the definition of the SVD and the norm, the orthogonality of $U$ and $V$ imply\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\|M-A\\right\\|^2 = \\left\\|U^{\\prime}(M-A) V\\right\\|^2 = \\left\\|\\Sigma-U^{\\prime} A V\\right\\|^2\n",
    "\\end{align*}\n",
    "\n",
    "Since $A$ is formulated to make $U^{\\prime} A V$ a diagonal matrix that agrees with the **some $k$ entries** of the diagonal matrix $\\Sigma$, the right hand side is just the squared norm of $\\Sigma$ after those $k$ diagonal entries have been zeroed out.\n",
    "\n",
    "For the Frobenius norm (whose square is the sum of squared entries of its argument), the **squared norm** of this zeroed-out copy of $\\Sigma$ is **the sum of squares of its remaining entries**, precisely\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\|\\Sigma-U^{\\prime} A V\\right\\|^2=\\sum_{j=k+1}^r \\delta_j^2 .\n",
    "\\end{align*}\n",
    "\n",
    "Thus, the **Frobenius norm of the reconstructing error is minimized** when the **singular values been zeroed-out are minimized**, that is, **removing the lowest singular values**.\n",
    "\n",
    "A formal proof can be found in [here](https://en.wikipedia.org/wiki/Low-rank_approximation#Proof_of_Eckart%E2%80%93Young%E2%80%93Mirsky_theorem_(for_Frobenius_norm))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47473f38",
   "metadata": {},
   "source": [
    "# Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ef054",
   "metadata": {},
   "source": [
    "The **Singular Value Decomposition (SVD)** is a **matrix factorization method** that can be used to produce **low-dimensional approximations** of high-dimensional matrices.\n",
    "\n",
    "The tutorial begins with the **definitions of SVD** and introduces the matrices $U$, $\\Sigma$, and $V$ with their properties. The tutorial then explains how to compute the SVD of a matrix and how this is related to the eigenvalues of the symmetric matrices $M^TM$ and $MM^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6156d34a",
   "metadata": {},
   "source": [
    "The tutorial also includes **examples of SVD applications**. The **first example is a rank-2 matrix** representing **movie ratings by users**, and it has two underlying features, **science-fiction and romance**. The matrix can be decomposed into $U$, $\\Sigma$, and $V$, and since the rank of M is 2, the decomposition is straightforward.\n",
    "\n",
    "The **second example has a rank of 3**. We can eliminate the third feature since it is insignificant to the dataset. In this section, we also learn how to **reduce dimensionality using SVD** by **setting the smallest of the singular values to zero**, which eliminates the corresponding columns of U and V."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d276bfb1",
   "metadata": {},
   "source": [
    "Finally, the tutorial explains **how SVD can help in movie and user recommendations** by making recommendations based on the assumption that if a person likes a particular movie, they will likely enjoy the same type of movie. The tutorial illustrates **how to make recommendations based on the first two features** in Example 3.\n",
    "\n",
    "In conclusion, SVD is a powerful tool that can be used to produce **low-dimensional approximations** of high-dimensional matrices. It facilitates the elimination of less important parts of the matrix representation, generating an approximate representation with **fewer dimensions**, and **simplifies the computations** required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "641px",
    "left": "24px",
    "top": "111.141px",
    "width": "357px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "64"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
